---
title: "Data-Driven Offensive Insights: Using Analytics to Improve Offensive Success Rates"
author: "Clayton Fogler"
advisor: "Dr. Matt Higham"
institution: "St. Lawrence University"  
date: "December 17, 2025"
format:
  pdf:
    toc: true
    urlcolor: blue
execute:
  echo: false
  warning: false
---

```{r SETUP}
#| include: false
#| eval: true

chunk_hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- chunk_hook(x, options)
  paste0("\\linespread{0.9}\n", x, "\n\n\\linespread{2}")
})
```

```{r echo = FALSE}
## Load libraries
library(tidyverse)
library(patchwork)
library(knitr)
library(doParallel)
library(foreach)
library(Metrics)
library(randomForest)
library(rpart.plot)
library(caret)
library(rpart)
library(ipred)
library(ranger)
library(h2o)

```

\clearpage

## Abstract

There are a lot of factors that go into winning football games. One of the biggest factors is an efficient offense, and at the core of an efficient offense is an offense with high success rates on their plays. This paper will investigate how a certain Division 3 football team can maximize their offensive success rates. Using data collected from their past two seasons, this study aims to examine how variables like Distance and Down can impact the chances of a successful play. After cleaning and organizing over 1,000 football plays, I created many models, using multiple methods, to determine the best way to predict success rates in the team's offense. Five models were built throughout the paper that each predict success rate based on a combination of the following variables: RP (run pass), DN (Down), DIST (Distance to go), and PERSONNEL (Offensive Personnel). To compare the models, I calculate a classification rate for each to see which model best predicts the team's success rates on offense. By identifying the most effective model, I will be able to highlight the most important factors that give the best chances for a successful play. This will provide further insights that can be used in live game-play calling to create a more efficient offense.

## Introduction

In this decade, we have seen major advancements in the use of analytics in sports. Whether used from a business perspective, player development prospective, or game play prospective, there are so many cool ways to use analytics to improve your team. In football, one of the greatest uses of analytics is finding tendencies in a team's playing schemes, measuring their efficiency, and finding a way to get the upper edge on the field. As a kicker on a small Division 3 football school, the use of analytics is small as stats are kept on a very minimal level. However, just because very little data is recorded, doesn't mean we can't find ways to use the data to improve our success.

The goal for my Senior Year Experience is to evaluate my teams' offensive success rates using both exploratory statistics, logistic regression analysis, classification trees, bagging, and random forest to improve our offensive efficiency. By studying the play by play data from both past and current seasons, I will be able to find patterns in both our play calling tendencies, our efficiency in specific situations, and potentially develop a program that simulates our drives.

Throughout this paper, I will use the term SUCCESS as the response variable. The variable SUCCESS refers to whether or not the offensive football play was successful or not. To figure out how to measure success, I went online and found multiple sources that defined success similarly. One website, titled Football Study Hall, defined success rate as "at least 40 - 50% of the yards to go on 1st down, at least 50-70% of yards to go on second down, and first down achievement on third and fourth down" (C, 2012). To get a first down achievement on third and fourth down, an offense must gain 100% of the yards to go. For the purpose of my paper, I defined success as needing 40% of the yards to go on 1st Down, 60% of the yards to go on second down, and 100% of the yards to go on third and fourth down.

This research will directly help the team as identifying our tendencies and efficiency levels will tell us our strengths and weaknesses, improving our play designs, playbook, and player development. By using our different model methods, along with our exploratory research, we will be able to build future models that can be updated as the season moves on to see if our changes are working. This will help our coaching staff handle uncertainties when trying to game plan for the game.

My project will begin with basic exploratory data analysis based off the last two years of games to get an understanding for the offense and find early trends. Things we will look at include success rates based on run/pass, distance, down, and personnel. We will then use these findings to build some logistic regression models before diving into classification trees, bagging, and random forests. In the end, we will compare our models to determine which one is the best at predicting successful football plays.

## Data Gathering and Tidying

Before discussing how we tidied our data, we must begin to explain how it was gathered. Like most teams at the division 3 football level, we used a software called Hudl to keep track of game film and statistics. It is a subscription-based website in which teams can upload film from games and practice, along with data such as down, distance (distance to go), personnel, and more. After recording plays and uploading them to hudl, the coaching staff will then go in and enter all of the information related to each play. Thankfully, Hudl has an option for coaches to download the statistics as an Excel file, that can then be transformed into a csv file.

After uploading each game, I combined all the data into one data set called `all_seasons`. This data set includes a lot of variables, however, the main variables that are used throughout the paper include:

-   DN (stands for what down the play is. 0 stands for the first down to begin a drive, 1 for 1st down, 2 for 2nd down, 3 for 3rd down, and 4 for 4th down.)

-   DIST (stands for the distance to go)

-   RP (whether the play was a run or a pass. R for run and P for pass.)

-   PERSONNEL (what players were on the field. With numbered personnel, the two numbers are used to tell the offense how many tight ends and running backs will be on the field. After factoring 5 offensive lineman and 1 quarterback, this means there are 5 remaining players allowed on the field. The first digit refers to the number of Tight Ends on the field. The second digit refers to the number of running backs on the field. Finally, if those two digits do not add to 5, the remaining number of players are wide receivers. For example, 12 personnel means their is 1 tight end, 2 running backs, and 2 wide receivers on the field. 11 personnel means their is 1 tight end, 1 running back, and 3 wide receivers on the field. With named personnel's, this usually refers to a specific game plan package that is unique to certain players.)

-   SUCCESSFUL (whether or not the play was successful. We defined SUCCESSFUL in the introduction).

-   SITUATION (Situation was used in our exploratory section to help look at the distances to go a little easier. Short means the DIST was between 1-3 yards. Medium meant 4-7 yards, Long meant 8-10 yards, and very long meant 11+ yards).

With our data collected and gathered, it was time to begin some exploratory research to see if we notice any early patterns in SUCCESS.

## Exploratory Research

```{R data entry}
endicott_2024 <- read_csv(here::here("data/endicott_2024.csv"))
norwich_2024 <- read_csv(here::here("data/norwich_2024.csv"))
wne_2024 <- read_csv(here::here("data/wne_2024.csv"))
union_2024 <- read_csv(here::here("data/union_2024.csv"))
rpi_2024 <- read_csv(here::here("data/rpi_2024.csv"))
hobart_2024 <- read_csv(here::here("data/hobart_2024.csv"))
rochester_2024 <- read_csv(here::here("data/rochester_2024.csv"))
ithaca_2024 <- read_csv(here::here("data/ithaca_2024.csv"))
buffstate_2024 <- read_csv(here::here("data/buffstate_2024.csv"))

norwich_2025 <- read_csv(here::here("data/norwich_2025.csv"))
fisher_2025 <- read_csv(here::here("data/fisher_2025.csv"))
alfred_2025 <- read_csv(here::here("data/alfred_2025.csv"))
union_2025 <- read_csv(here::here("data/union_2025.csv"))
hilbert_2025 <- read_csv(here::here("data/hilbert_2025.csv"))
ithaca_2025 <- read_csv(here::here("data/ithaca_2025.csv"))
hobart_2025 <- read_csv(here::here("data/hobart_2025.csv"))
rpi_2025 <- read_csv(here::here("data/rpi_2025.csv"))
buffstate_2025 <- read_csv(here::here("data/buffstate_2025.csv"))
rochester_2025 <- read_csv(here::here("data/rochester_2025.csv"))
```

```{R tidy1}
union_2024 <- union_2024 |>
  select(-YEAR)

endicott_2024 <- endicott_2024[-46, ]

```

```{R tidy2}
endicott_2024 <- endicott_2024 |>
  mutate(OPPONENT = "endicott", YEAR = "2024") 
  
norwich_2024 <- norwich_2024 |>
  mutate(OPPONENT = "norwich", YEAR = "2024") 

wne_2024 <- wne_2024 |>
  mutate(OPPONENT = "wne", YEAR = "2024") 

union_2024 <- union_2024 |>
  mutate(OPPONENT = "union", YEAR = "2024") 

rpi_2024 <- rpi_2024 |>
  mutate(OPPONENT = "rpi", YEAR = "2024") 

hobart_2024 <- hobart_2024 |>
  mutate(OPPONENT = "hobart", YEAR = "2024") 

rochester_2024 <- rochester_2024 |>
  mutate(OPPONENT = "rochester", YEAR = "2024") 

ithaca_2024 <- ithaca_2024|>
  mutate(OPPONENT = "ithaca", YEAR = "2024") 

buffstate_2024 <- buffstate_2024 |>
  mutate(OPPONENT = "buffstate", YEAR = "2024") 

norwich_2025 <- norwich_2025 |>
  mutate(OPPONENT = "norwich", YEAR = "2025")

fisher_2025 <- fisher_2025 |>
  mutate(OPPONENT = "fisher", YEAR = "2025")

alfred_2025 <- alfred_2025 |>
  mutate(OPPONENT = "alfred", YEAR = "2025")

union_2025 <- union_2025 |>
  mutate(OPPONENT = "union", YEAR = "2025")

hilbert_2025 <- hilbert_2025 |>
  mutate(OPPONENT = "hilbert", YEAR = "2025")

ithaca_2025 <- ithaca_2025 |>
  mutate(OPPONENT = "ithaca", YEAR = "2025")

hobart_2025 <- hobart_2025 |>
  mutate(OPPONENT = "hobart", YEAR = "2025")

rpi_2025 <- rpi_2025 |>
  mutate(OPPONENT = "rpi", YEAR = "2025")

buffstate_2025 <- buffstate_2025 |>
  mutate(OPPONENT = "buffstate", YEAR = "2025")

rochester_2025 <- rochester_2025 |>
  mutate(OPPONENT = "rochester", YEAR = "2025")

```

```{R data combining}
all_seasons <- rbind(endicott_2024, norwich_2024, wne_2024, union_2024, rpi_2024, hobart_2024, rochester_2024, ithaca_2024, buffstate_2024, norwich_2025, fisher_2025, alfred_2025, union_2025, hilbert_2025, ithaca_2025, hobart_2025, rpi_2025, buffstate_2025, rochester_2025)

all_seasons <- all_seasons |>
  select(-FORMATION)
```

```{R tidy3}
 all_seasons <- all_seasons |>
    mutate(
      SUCCESSFUL = case_when(
        DN == 0 & `GN/LS` >= (0.4 * DIST) ~ 1,                     
        DN == 1 & `GN/LS` >= (0.4 * DIST) ~ 1,
        DN == 2 & `GN/LS` >= (0.6 * DIST) ~ 1,
        DN == 3 & `GN/LS` >= (DIST) ~ 1,
        DN == 4 & `GN/LS` >= (DIST) ~ 1,
        TRUE ~ 0
      )
    ) |>
    mutate(
      SITUATION = case_when(
        DIST <= 3 ~ "Short (1-3)",
        DIST <= 6 ~ "Medium (4-6)",
        DIST <= 10 ~ "Long (7-10)",
        TRUE ~ "Very Long (11+)"
      )
    ) |>
    rename(
      RP = `R/P`,
      GL = `GN/LS`,
      FORMATION = `OFF FORM`,
      RESULT = `PLAY RESULT`
    ) |>
    select(
      DN, DIST, HASH, GL, RP, FORMATION, BACKFIELD, RESULT, PERSONNEL, OPPONENT, YEAR, SUCCESSFUL, SITUATION
    ) |>
    drop_na(
      RP, DIST, DN, PERSONNEL
    ) 
```

To begin my exploratory research, I wanted to create an assortment of graphs and charts to help visualize my data and see if any early trends arise. I created a chart for each one of my predictors to see how success related to each of them:

Success based on RP

```{r success by rp}
success_by_rp <- all_seasons |>
  group_by(RP) |>
  summarise(
    plays = n(),
    `Success Rate` = paste0(round(mean(SUCCESSFUL, na.rm = TRUE) * 100, 1), " %")
  ) |>
  arrange(RP, desc(`Success Rate`))

success_by_rp
```

Our offense appeared to have a little more success passing the ball then running, however, it is not by much. This shows potential signs of RP not being the strongest predictor, especially on its own.

Success based on DN

```{r success by DN}
success_by_dn <- all_seasons |>
  group_by(DN) |>
  summarise(
    plays = n(),
    `Success Rate` = paste0(round(mean(SUCCESSFUL, na.rm = TRUE) * 100, 1), " %")
  ) |>
  arrange(DN)

success_by_dn
```

Success rate on Third Downs seems to be a lot less compared to the other Downs. This shows that downs has the potential to be a strong predictor.

Success based on DIST

```{r success by DIST/SITUATION}
success_by_dist <- all_seasons |>
  group_by(SITUATION) |>
  summarise(
    plays = n(),
    `Success Rate` = paste0(round(mean(SUCCESSFUL, na.rm = TRUE) * 100, 1), " %")
  ) |>
  arrange(desc(`Success Rate`))

success_by_dist
```

Because there are so many different Distances that occurred over the past two seasons, I created the variable `SITUATION` to help visualize the effect of distance on success. Situation is broken into 4 categories: Short (distance = 1-3 yards), Medium (distance = 4-6 yards), Long (distance = 7-10 yards), and Very Long (distance = 11+ yards). We see above that distance has a huge impact on the success rate, with an over 20% decrease between Long and Very Long. Early on, DIST seems to be the strongest predictor of SUCCESS.

Success based on PERSONNEL

```{r success by personnel}
success_by_personnel <- all_seasons |>
  group_by(PERSONNEL) |>
  summarise(
    plays = n(),
    `Success Rate` = paste0(round(mean(SUCCESSFUL, na.rm = TRUE) * 100, 1), " %")
  ) |>
  arrange(PERSONNEL)

success_by_personnel
```

We notice that there are some large jumps in success rate between personnel groupings, however, there is also a large difference in play count for each. 11 personnel (1 running back, 1 tight end, 3 wide receivers) was used 747 times, with the next closest being 12 personnel (1 running back, 2 tight ends, 2 wide receivers) with 153 plays. The lack of even plays will probably limit the effect personnel has on predicting success.

We can also visualize multiple variables at once, using various heat maps to try to find some correlation between variables. Because we have early evidence that DIST will be our strongest predictor, lets compare the other 3 variables with Situation (our modified distance variable).

Situation and RP

```{r situation and rp heat map}
success_map_sit_rp <- all_seasons |>
  mutate(
    SITUATION = as.factor(SITUATION),
    RP = as.factor(RP)
  ) |>
  group_by(SITUATION, RP, .drop = FALSE) |>
  summarise(
    SUCCESS = mean(SUCCESSFUL, na.rm = TRUE), 
    n = n()                                    
  ) |>
  ungroup() |>
  mutate(
    # re-code situations for nicer labels
    SITUATION = fct_recode(SITUATION,
                           "Short (1-3)" = "1",
                           "Medium (4-6)" = "2",
                           "Long (7-10)" = "3",
                           "Very Long (11+)" = "4")
  )

ggplot(success_map_sit_rp, aes(x = SITUATION, y = RP, fill = SUCCESS)) +
  geom_tile(colour = "grey") +
  geom_text(aes(label = n), color = "white", fontface = "bold") +
  scale_fill_viridis_c(
    name = "Success Rate",
    labels = scales::percent
  ) +
  theme_minimal() +
  scale_x_discrete(position = "top") +
  labs(x = "Situation", y = "Run / Pass")
```

Situation and DN

```{r situation and dn heat map}
success_map <- all_seasons |>
  mutate(DN = as.factor(DN),
         SITUATION = as.factor(SITUATION)) |>
  group_by(SITUATION, DN, .drop = FALSE) |>
  summarise(
    SUCCESS = mean(SUCCESSFUL),
    n = n()) |>
  ungroup() |>
  mutate(
    # recode situation for nicer labels
    SITUATION = fct_relevel(SITUATION, c(
                           "Short (1-3)" = "1",
                           "Medium (4-6)" = "2",
                           "Long (7-10)" = "3",
                           "Very Long (11+)" = "4")),
    # order downs from 1st to 4th
    DN = fct_relevel(DN, c("0","1","2","3","4"))
  )

ggplot(success_map, aes(x = SITUATION, y = DN, fill = SUCCESS)) +
  geom_tile(colour = "grey") +
  geom_text(aes(label = n), color = "white", fontface = "bold") +
  scale_fill_viridis_c(name = "Success Rate", labels = scales::percent) +
  theme_minimal() +
  scale_x_discrete(position = "top")
```

Situation and Personnel

```{r situation and personnel heat map}
success_map <- all_seasons |>
  mutate(PERSONNEL = as.factor(PERSONNEL),
         SITUATION = as.factor(SITUATION)) |>
  group_by(PERSONNEL, SITUATION, .drop = FALSE) |>
  summarise(
    SUCCESS = mean(SUCCESSFUL),
    n = n()) |>
  ungroup() |>
  mutate(
    # recode situation for nicer labels
    SITUATION = fct_recode(SITUATION,
                           "Short (1-3)" = "1",
                           "Medium (4-6)" = "2",
                           "Long (7-10)" = "3",
                           "Very Long (11+)" = "4"),
  )


ggplot(success_map, aes(x = SITUATION, y = PERSONNEL, fill = SUCCESS)) +
  geom_tile(colour = "grey") +
  geom_text(aes(label = n), color = "white", fontface = "bold") +
  scale_fill_viridis_c(name = "Success Rate", labels = scales::percent) +
  theme_minimal() +
  scale_x_discrete(position = "top")
```

The least effective graph above was our graph displaying the relationship between personnel and situation. Again, this isn't a surprise as there were 14 different personnel groupings recorded over the last two seasons, with 11 personnel being used over 700 times. However, this doesn't mean it will be a bad or ineffective variable. We will learn more later on in our model building if personnel is worth having. While a RP and DN definitely have relationships with Success Rate, it is clear to see with these graphs that the strongest relationship is the relationship between SITUATION and SUCCESS. After doing some exploratory research, it is time to dive into our model building.

## Modeling Methods

### Method 1: Logistic Regression

```{r creating a data set for the glm models}
all_seasons_logit <- all_seasons |>
  mutate(
    RP = factor(RP),
    DN = factor(DN),
    PERSONNEL = factor(PERSONNEL),
    SUCCESSFUL = factor(SUCCESSFUL)
  ) |>
  
  select(
    SUCCESSFUL, RP, DIST, DN, PERSONNEL
  )
```

To start off our model building, I wanted to use a logistic regression model. Logistic regression models are nice because they allow us to look at the relationship between a response variable and however many predictor variables we want to use. Because we are modeling SUCCESS, a categorical variable, we use Logistic Regression instead of Linear. My first step was to build 4 models, one for each variable that I am interested in using: DN, DIST, PERSONNEL, and RP:

```{r building our base glm models, echo=TRUE}
logRP <- glm(SUCCESSFUL ~ RP, data = all_seasons_logit, family = binomial)
AIC(logRP)

logDIST <- glm(SUCCESSFUL ~ DIST, data = all_seasons_logit, family = binomial)
AIC(logDIST)

logDN <- glm(SUCCESSFUL ~ DN, data = all_seasons_logit, family = binomial)
AIC(logDN)

logPERSONNEL <- glm(SUCCESSFUL ~ PERSONNEL, data = all_seasons_logit, family = binomial)
AIC(logPERSONNEL)
```

With these starter models, the model with the lowest AIC was logDIST (AIC = 1442.7), which uses the DIST variable to predict SUCCESSFUL. We saw a strong relationship with DIST and SUCCESS in our exploratory section, so it makes sense as to why that model had the best AIC. However, we know as we begin to add more variables, we will probably start to see better models. The best way to find the best combination of variables for a logistic regression model would be to create a model with all of our variables, and then use the `step()` function to find which model is the best.

```{r building a glm model with all variabes, echo=FALSE}
logALL <- glm(SUCCESSFUL ~ RP + DIST + DN + PERSONNEL, data = all_seasons_logit, family = binomial)

logBEST <- step(logALL, direction = "backward")
AIC(logBEST)
```

LogBEST returned back a model with all four variables with an AIC of 1414.8, which was better than our logDIST model. However, I noticed in the coding output that a model with DN, DIST, and RP (no PERSONNEL) got the same AIC of 1414.8. To decide whether or not to keep the PERSONNEL term, I compared the two models BIC and found that the model without PERSONNEL resulted in a lower BIC. Because of this, my final predictor variables are DN, DIST, and RP.

Next, I used the `train()` function to build the model again, but added 5 fold cross validation. The biggest reason for doing this was to be able to put every model I build through the same validation process to make them easy to compare.

```{r chaging successful to yes/no}
all_seasons_logit$SUCCESSFUL <- factor(all_seasons_logit$SUCCESSFUL,
                                       levels = c(0, 1),
                                       labels = c("No", "Yes"))

```

```{r rebuilding the model with cross validation}
set.seed(123)

logFinal <- train(
  SUCCESSFUL ~ RP + DIST + DN,  
  data = all_seasons_logit,                
  method = "glm",                           
  family = binomial,                       
  trControl = trainControl(
    method = "cv",                         
    number = 5,                            
    savePredictions = "final",              
    classProbs = TRUE                    
  )
)

```

```{r classification rate calc 1, echo=TRUE}
# Extract cross-validated predictions
cv_preds <- logFinal$pred

# Convert probability of "Yes" to predicted class
cv_preds$pred_class <- ifelse(cv_preds$Yes > 0.5, "Yes", "No")

# Actual outcomes
actual <- cv_preds$obs

# Compute classification rate
classification_rate <- mean(cv_preds$pred_class == actual)
classification_rate

```

I then calculated the classification rate for this model. Using DN (Down), DIST (Distance to go), and RP (whether the play was a run or pass) to predict SUCCESS resulted in a classification rate of 0.602, meaning our model correctly predicted 60.2% of the actual plays. I wanted to also visualize this model so I built the following graph.

```{r predicted probaility of success graph}

most_common_DN <- names(sort(table(all_seasons_logit$DN), decreasing = TRUE))[1]

dist_values <- sort(unique(all_seasons_logit$DIST))
RP_levels <- levels(all_seasons_logit$RP)

dist_grid <- expand.grid(
  DIST = dist_values,
  RP = RP_levels,
  DN = most_common_DN
)

# Use caret's predict method
dist_grid$pred_prob <- predict(logFinal, newdata = dist_grid, type = "prob")[, "Yes"]

# Plot
ggplot(dist_grid, aes(x = DIST, y = pred_prob, color = RP)) +
  geom_line(size = 1) +
  labs(
    x = "Distance",
    y = "Predicted Probability of Success",
    color = "RP Level",
    title = "Predicted Probability of Success by DIST and RP"
  ) +
  theme_minimal()

```

In the graph, we notice that run and pass are fairly parallel, as they begin to get closer as we approach a distance greater than 15 yards. It is important to note that there is no overlap in the two slopes (this will be important to remember for later).

To be correct 60.2% of the time is fairly good, however, there are many methods that can be used to predict outcomes. Lets now try Logistic Regression Models that include interaction terms.

### Method 2: Logistic Regression with Interaction

```{r re-setting our dataset}
all_seasons_logit <- all_seasons |>
  mutate(
    RP = factor(RP),
    DN = factor(DN),
    PERSONNEL = factor(PERSONNEL),
    SUCCESSFUL = factor(SUCCESSFUL)
  ) |>
  select(
    SUCCESSFUL, RP, DIST, DN, PERSONNEL
  )
```

To begin our logistic regression model with interaction, we again build a model with not only all the predictors, but all of the possible interactions as well.

```{r building an glm model including interaction, echo=TRUE}
logALL_inter <- glm(SUCCESSFUL ~ (RP  + DIST + DN + PERSONNEL)^2, data = all_seasons_logit, family = binomial)
```

We then use the step function to find the best possible model.

```{r step function to find best model, echo=FALSE}
logBEST_inter <- step(logALL_inter, direction = "both", trace = FALSE)
summary(logBEST_inter)
```

It seems that the best possible model using our 4 variables to predict success, based on AIC and using interaction terms, is a model with RP, DIST,DN, PERSONNEL, and RP:DIST interaction(AIC = 1406).

Next, I used the `train()` function again to build the model using 5 fold cross validation.

```{r adjusting the data for train function}
all_seasons_logit$SUCCESSFUL <- factor(all_seasons_logit$SUCCESSFUL,
                                       levels = c(0, 1),
                                       labels = c("No", "Yes"))

```

```{r building our interaction model with cross validation}
set.seed(123)

LOGINTERACTION <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL + RP*DIST,
  data = all_seasons_logit,                
  method = "glm",                           
  family = binomial,                      
  trControl = trainControl(
    method = "cv",                         
    number = 5,                            
    savePredictions = "final",             
    classProbs = TRUE                 
  )
)

```

```{r cross validation 2, echo=TRUE}
# Extract cross-validated predictions
cv_preds_int <- LOGINTERACTION$pred

# Convert probability of "Yes" to predicted class
cv_preds_int$pred_class <- ifelse(cv_preds_int$Yes > 0.5, "Yes", "No")

# Actual outcomes
actual_int <- cv_preds_int$obs

# Compute classification rate
classification_rate2 <- mean(cv_preds_int$pred_class == actual_int)
classification_rate2

```

After the model goes through 5 fold cross-validation, I can now calculate the classification rate for this model. A model using DN, DIST, RP, PERSONNEL, and an interaction with RP and DIST to predict SUCCESS resulted in a classification rate of 0.616, meaning our model correctly predicted 61.6% of the actual plays. I wanted to also visualize this model so I built the following graph.

```{r second graph of predicted successes, this time with interaction terms}

most_common_DN <- names(sort(table(all_seasons_logit$DN), decreasing = TRUE))[1]
most_common_PERSONNEL <- names(sort(table(all_seasons_logit$PERSONNEL), decreasing = TRUE))[1]

dist_values <- sort(unique(all_seasons_logit$DIST))
RP_levels <- levels(all_seasons_logit$RP)

dist_grid <- expand.grid(
  DIST = dist_values,
  RP = RP_levels,
  DN = most_common_DN,
  PERSONNEL = most_common_PERSONNEL
)

# Use caret's predict method
dist_grid$pred_prob <- predict(LOGINTERACTION, newdata = dist_grid, type = "prob")[, "Yes"]

# Plot
ggplot(dist_grid, aes(x = DIST, y = pred_prob, color = RP)) +
  geom_line(size = 1) +
  labs(
    x = "Distance",
    y = "Predicted Probability of Success",
    color = "RP Level",
    title = "Logistic Regression Predicted Probability by DIST and RP (Interaction Model)"
  ) +
  theme_minimal()

```

The biggest different between our first two methods so far is our graph of the predicted probabilities of success. In our graph with interaction, we see that the fitted curves of run and pass cross when distance is at about 5 yards. This means our interaction model is seeing a shift in which pass plays are more successful in longer distances. We will do more comparisons at the end, however, we do begin to see that a model with interaction is a little better at predicting success than our model without interaction.

### Method 3: Classification Trees

```{r trees data set}
all_seasons_trees <- all_seasons |>
  mutate(
    RP = factor(RP),
    DN = factor(DN),
    PERSONNEL = factor(PERSONNEL),
    SUCCESSFUL = factor(SUCCESSFUL)
  ) |>
  select(
    SUCCESSFUL, RP, DIST, DN, PERSONNEL
  )
```

```{r fixing data for train}
all_seasons_trees$SUCCESSFUL <- factor(all_seasons_trees$SUCCESSFUL,
                                       levels = c(0, 1),
                                       labels = c("No", "Yes"))

```

Logistic regression is not the only method that can be used in this scenario. Another effective method that can be used are classification trees. Classification trees is a kind of model that starts with one yes or no question that then breaks off into branches of yes or no questions that best predict what your model is trying to predict. For this paper, the classification tree is trying to find what pathways the data best follows to predict whether a play was successful or not. Our model is as follows:

```{r 5 fold cross validation, echo=TRUE}
ctrl <- trainControl(
  method = "cv",   
  number = 5,       
  classProbs = TRUE, 
  summaryFunction = twoClassSummary 
)

```

```{r full classification tree, echo=TRUE}
set.seed(123) 

tree <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL,
  data = all_seasons_trees,
  method = "rpart",
  trControl = ctrl,
  metric = "ROC", 
  tuneLength = 10
)

```

To help further understand what a classification tree looks like, here is the classification tree for my model. Note that this isn't my exact classification tree as my tree has too many branches to understand what is going on. I created a second version of my model that caps the tree depth to make the chart easier to see and understand.

```{r partial classification tree, echo=FALSE}
set.seed(123) 

partialtree <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL,
  data = all_seasons_trees,
  method = "rpart",
  trControl = ctrl,
  metric = "ROC", 
  tuneLength = 10,
  control = rpart.control(maxdepth = 5)
)

rpart.plot(partialtree$finalModel, extra = 104)

```

In our partial tree, you see that the first partition is distance, showing that distance was the strongest predictor out of our variables. It is also interesting to note that personnel ranks higher than some of the other variables, which is not something I would've predicted based on our exploratory research.

```{r classification rate 3, echo=TRUE}
pred_tree <- predict(tree, newdata = all_seasons_trees)

actual_tree <- all_seasons_trees$SUCCESSFUL

classification_rate3 <- mean(pred_tree == actual_tree)

classification_rate3

```

Since we already built our model with 5 fold cross-validation, we can jump right into finding our classification rate. Classification Rate for our Classification Tree is 0.665, meaning our model correctly predicted 66.5% of the actual plays. This is better than both our base logistic regression model, and our logistic regression model with interaction. Before we do a full comparison of all our models, we will build our final two models using bagging and random forest.

### Method 4: Bagging and Random Forest

```{r creating the bagging data}
all_seasons_bagging <- all_seasons |>
    mutate(
      SUCCESSFUL = case_when(
        DN == 0 & GL >= (0.4 * DIST) ~ "YES",                     
        DN == 1 & GL >= (0.4 * DIST) ~ "YES",
        DN == 2 & GL >= (0.6 * DIST) ~ "YES",
        DN == 3 & GL >= (DIST) ~ "YES",
        DN == 4 & GL >= (DIST) ~ "YES",
        TRUE ~ "NO"
      )
    )
```

```{r making sure every variable is a factor as needed}
`all_seasons_bagging` <-`all_seasons_bagging` |>
  mutate(
    SUCCESSFUL = as.factor(SUCCESSFUL),
    RP = as.factor(RP),
    PERSONNEL = as.factor(PERSONNEL),
    DN = as.factor(DN),
  ) |>
  select(
    SUCCESSFUL, RP, PERSONNEL, DN, DIST
  )
```

Classification trees can be really effective, however, they can sometimes be very sensitive to changes in the training data, as well as overfitting. That is where bootstrapping comes in. Bootstrapping repeatedly re-samples with replacement your original data, and creates new training data. Then, it runs a new classification tree based on the training data. It will do this as many times as you would like, helping reduce the variance in your model. Our bagging model is as follows:

```{r building our bagging model, echo=TRUE}
set.seed(123)

football_bagging <- train( 
  SUCCESSFUL ~ `RP` + DIST + DN + PERSONNEL,
  data = all_seasons_bagging, 
  method = "treebag", 
  trControl = trainControl(method = "cv", number = 5), # cv = cross validation, number is amount of folds
  nbagg = 200,  #the number of trees being built
  control = rpart.control(minsplit = 2, cp = 0) #
)

```

```{r classification rate 4, echo=TRUE}
# predicted class
pred_class_bagging <- predict(football_bagging, type = "raw")

# actual outcomes
actual_bagging <- all_seasons_bagging$SUCCESSFUL

# classification rate
classification_rate_bagging <- mean(pred_class_bagging == actual_bagging)
classification_rate_bagging


```

We then can use this model and calculate our classification rate, which we get to be 0.724. This means our model correctly predicted 72.4% of the actual plays. This is a big jump from our classification tree method. Lets see if random forest makes a similar jump.

Random forest

```{r random forrest model building, echo=TRUE}
ctrl2 <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  allowParallel = FALSE   
)


set.seed(123)

football_rf <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL,
  data = all_seasons_bagging,
  method = "ranger",
  trControl = ctrl2,
  metric = "ROC",
  tuneLength = 10
)

```

```{r}
# predicted class
pred_forest <- predict(football_rf, data = all_seasons_bagging)

# actual outcomes
actual_forest <- all_seasons_bagging$SUCCESSFUL

# classification rate
classification_rate_rf <- mean(pred_forest == actual_forest)
classification_rate_rf
```

For our random forest model, we found a classification rate of 0.667. This means our model correctly predicted 66.7% of the actual plays.

## Comparing the Four Methods

To easily compare our 5 classification rates we have found, I created the following graph:

```{r classifcation table with all our rates}
classification_table <- data.frame(
  Method = c(
    "Logistic Regression",
    "Logistic Regression with Interaction",
    "Classification Trees",
    "Bagging",
    "Random Forest"
  ),
  Classification_Rate = c(
    classification_rate,
    classification_rate2,
    classification_rate3,
    classification_rate_bagging,
    classification_rate_rf
  )
)

ggplot(classification_table,
       aes(x = reorder(Method, Classification_Rate),
           y = Classification_Rate)) +
  geom_col(fill = "purple1") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent, limits = c(0,1)) +
  labs(
    x = "",
    y = "Classification Rate",
    title = "Comparing CR by Model"
  ) +
  theme_minimal(base_size = 12)

```

As you can see, the far away best model we built was our bagging model. We notice that logistic regression, with or without interaction, were the worst two models. This isn't because GLM models are not as good as bagging and random forest, but instead means that the data is not best fit for a linear model. Because we have a strong interaction between RP and DIST, as well as non-linear patterns in the log-odds for success, a linear model like our logistic regression model does not best fit our data. Instead, re-sampling methods like classification trees, bagging, and random forest will have more success capturing the complexities of our data.

## Conclusion

Throughout this model, we see time and again Distance be the strongest predictor of success. This makes a ton of sense as a shorter distance to go is much more obtainable than a longer distance to go. While this project is done based on a single team's data, Dist should be the strongest or one of the strongest predictors of success with whatever school you applied this research to. We also saw downs, specifically 3rd down, be one of the stronger predictors of success. 3rd downs are a pivotal part of a game, as third downs can either extend your drives and bring more chances for successful drives, or end your drives and kill the momentum. A model built using bagging had the greatest success of predicting success. With its re-sampling and simulation ability, the bagging model better fit the data strongly outperformed the others. The model correctly predicted 72.4% of the plays, which is a very successful rate. Using this model and my research, I hope to communicate with the coaches of the team about a plan for next season to incorporate our findings. As a staff, the team needs to find a way to focus on creating short down situations, especially on 3rd downs.

## References

C, B. (2012, February 16). In Defense Of Success Rates. Football Study Hall.        
    https://www.footballstudyhall.com/2012/2/16/2798555/in-defense-of-success-ra
    tes?utm_source=chatgpt.com


