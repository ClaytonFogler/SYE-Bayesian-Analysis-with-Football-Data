---
title: "2024_Season"
author: "Fogler"
format: html
editor: visual
---

```{r}
library(tidyverse)
library(patchwork)
```

```{r}
# Helper packages
library(tidyverse)
library(doParallel)
library(foreach)
library(Metrics)
library(randomForest)

#Modeling packages
library(rpart.plot)
library(caret)
library(rpart)
library(ipred)

library(ranger)
library(h2o)
```

*Our first method of data analysis: Logistic Regression:*

*We will build a ton of models and use AIC (expain the AIC here a little bit and why we are using AIC) to pick our best model. At the end, we will find a classification rate to make it comparable to our other methods to come*

*First, we will make a version of our `all_seasons` data that makes sure to factor the right variables:*

```{r}
all_seasons_logit <- all_seasons |>
  mutate(
    RP = factor(RP),
    DN = factor(DN),
    PERSONNEL = factor(PERSONNEL),
    SUCCESSFUL = factor(SUCCESSFUL)
  ) |>
  
  drop_na(
    RP, DIST, DN, PERSONNEL
    ) |>
  
  select(
    SUCCESSFUL, RP, DIST, DN, PERSONNEL
  )
```

*Start with 4 simple models, predicting successful based individually on our 4 selected variables: RP, DN, PERSONNEL, DIST*

```{r}
logRP <- glm(SUCCESSFUL ~ RP, data = all_seasons_logit, family = binomial)
summary(logRP)

logDIST <- glm(SUCCESSFUL ~ DIST, data = all_seasons_logit, family = binomial)
summary(logDIST)

logDN <- glm(SUCCESSFUL ~ DN, data = all_seasons_logit, family = binomial)
summary(logDN)

logPERSONNEL <- glm(SUCCESSFUL ~ PERSONNEL, data = all_seasons_logit, family = binomial)
summary(logPERSONNEL)
```


*With out basic models, we find that the model with the best AIC was a logistic regression model predicting success based on Distance (with an AIC of 1442.7). Now, lets build more complex logistic regression models with multiple predictors in each model.*

```{r}
logALL <- glm(SUCCESSFUL ~ RP + DIST + DN + PERSONNEL, data = all_seasons_logit, family = binomial)
summary(logALL)
```

*Now, we can use the stepwise function to sort through all of the combinations of our 5 variables to see which model would be best:*

```{r}
logTEST <- glm(SUCCESSFUL ~ RP + DIST + DN, data = all_seasons_logit, family = binomial)
summary(logTEST)
BIC(logTEST)
```



```{r}
logBEST <- step(logALL, direction = "both", trace = TRUE)
summary(logBEST)
BIC(logBEST)
```

*It seems that the best possible model using our 4 variables to predict success, based on AIC, is a model with RP, DIST, and DN (AIC of 1414.8, BIC of 1514.917).*


```{r}
all_seasons_logit$SUCCESSFUL <- factor(all_seasons_logit$SUCCESSFUL,
                                       levels = c(0, 1),
                                       labels = c("No", "Yes"))

```

```{r}
LOGBEST <- train(
  SUCCESSFUL ~ RP + DIST + DN,  # your formula
  data = all_seasons_logit,                 # dataset
  method = "glm",                           # logistic regression
  family = binomial,                        # logistic model
  trControl = trainControl(
    method = "cv",                          # k-fold cross-validation
    number = 5,                             # 5 folds
    savePredictions = "final",              # save predictions for held-out folds
    classProbs = TRUE                       # get predicted probabilities
  )
)

```

Before we start using interaction terms, lets see what the classification rate is for our model we just found:

```{r}
# Extract cross-validated predictions
cv_preds <- LOGBEST$pred

# Convert probability of "Yes" to predicted class
cv_preds$pred_class <- ifelse(cv_preds$Yes > 0.5, "Yes", "No")

# Actual outcomes
actual <- cv_preds$obs

# Compute classification rate
classification_rate <- mean(cv_preds$pred_class == actual)
classification_rate

```

Our logistic Regression Model using DN (Down), DIST (Distance to go), and RP (whether the play was a run or pass) resulted in a classification rate of 0.618, meaning our model correctly predicted 61.8% of the plays.


For fun and visualization, we can graph our predicted values based off our logistic regression model:


```{r}

most_common_DN <- names(sort(table(all_seasons_logit$DN), decreasing = TRUE))[1]

dist_values <- sort(unique(all_seasons_logit$DIST))
RP_levels <- levels(all_seasons_logit$RP)

dist_grid <- expand.grid(
  DIST = dist_values,
  RP = RP_levels,
  DN = most_common_DN
)

# Use caret's predict method
dist_grid$pred_prob <- predict(LOGBEST, newdata = dist_grid, type = "prob")[, "Yes"]

# Plot
ggplot(dist_grid, aes(x = DIST, y = pred_prob, color = RP)) +
  geom_line(size = 1) +
  labs(
    x = "Distance",
    y = "Predicted Probability of Success",
    color = "RP Level",
    title = "Logistic Regression Predicted Probability by DIST and RP"
  ) +
  theme_minimal()

```












































