---
title: "Write Up"
author: "C.Fogler"
format: html
editor: visual
---

#Cover page for the paper, still need to figure out formatting

#TITLE OF THE PAPER

Clayton Fogler

Advisor: Dr. Matt Higham

St.Lawrence University

Department of Math, Computer Science, Statistics, and Data Science

December 15th, 2025

Table of contents: pg.2

Will be able to complete contents once paper is otganized and finalized

## Abstract : Add an Abstract section here at the end

## Introduction : Section 1

In this decade, we have seen major advancements in the use of analytics in sports. Whether it is used from a business prospective, player development prospective, or game play prospective, there are so many cool ways to use analytics to improve your team. In football, one of the greatest uses of analytics is finding tendencies in a teams playing schemes, measuring their efficiency, and finding a way to get the upper edge off the field. As a kicker on a small division 3 football school, the use if analytics is very minimal as stats are kept on a very minimal level. However, just because very little data is recorded, doesn't mean we can't find ways to use the data to improve our success.

The goal for my Senior Year Experience is to evaluate my teams' offensive success rates using both exploratory statistics, logistic regression analysis, classification trees, bagging, and random forest to improve our offensive efficiency. By studying the play by play data from both past and current seasons, I will be able to find patterns in both our play calling tendencies, our efficiency in specific situations. and potentially develop a program that simulates our drives.

Throughout this paper, I will use the term SUCCESS as the response variable. The variable SUCCESS refers to whether or not the offensive football play was successful or not. To figure out how to measure success, I went online and found multiple sources that defined success similarly. One website, titled Football Study Hall, defined success rate as "at least 40 - 50% of the yards to go on 1st down, at least 50-70% of yards to go on second down, and first down achievement on third and fourth down" (https://www.footballstudyhall.com/2012/2/16/2798555/in-defense-of-success-rates?utm_source=chatgpt.com). To get a first down achievement on third and first down, an offense must gain 100% of the yards to go. For the purpose of my paper, I defined success as needing 40% of the yards to go on 1st Down, 60% of the yards to go on second down, and 100% of the yards to go on third down.

This research will directly help the team as identifying our tendencies and efficiencies will tell us our strengths and weaknesses, improving our play designs, playbook, and player development. By using our different model methods, along with our exploratory research, we will be able to build future models that can be updated as the season moves on to see if our changes are working. This will help our coaching staff handle uncertainties when trying to game plan for the game.

My project will begin with basic exploratory data analysis based off last years games to get an understanding for the offense and find early trends. Things we will look at include success rates based on formation, play type, situation, and more. We will then use these findings to build some logistic regression models before building our priors for our Bayesian analysis. Finally, we will use data from this years games to update our beliefs and find ways to improve our offensive efficiency.

## Data Gathering and Tidying

Before discussing how we tidied our data, we must begin explain how it was gathered. Like most team at the division 3 football level, we used a software called Hudl to keep track of game film and statistics. It is a subscription-based website in which teams can upload film from games and practice, along with data such as down, distance (distance to go), personnel, and more. After recording plays and uploading them to hudl, the coaching staff will then go in and enter all of the information related to each play. Thankfully, hudl has an option for coaches to download the statistics as an excel file, that can then be transformed into a csv file.

After uploading each game, I combined all the data into one data set called `all_seasons`. This data set includes a lot of variables, however, the main variables that are used throughout the paper inlude:

-   DN (stands for what down the play is. 0 stands for the first down to begin a drive, 1 for 1st down, 2 for 2nd down, 3 for 3rd down, and 4 for 4th down.)

-   DIST (stands for the distance to go)

-   RP (whether the play was a run or a pass. R for run and P for pass.)

-   PERSONNEL (what players were on the field. With numbered personnel's, the two numbers are used to tell the offense how many tight ends and running backs will be on the field. After factoring 5 offensive lineman and 1 quarterback, this means their are 5 remaining players allowed on the field. The first digit refers to the number of Tight Ends on the field. The second digit refers to the number of running backs on the field. Finally, if those two digits do not add to 5, the remaining number of players are wide receivers. For example, 12 personnel means their is 1 tight end, 2 running backs, and 2 wide receivers on the field. 11 personnel means their is 1 tight end, 1 running back, and 3 wide receivers on the field. With named personnel's, this usually refers to a specific game plan package that is unique to certain players.)

-   SUCCESSFUL (whether or not the play was successful. We defined SUCCESSFUl in the introduction).

-   SITUATION (Situation was used in our exploratory section to help look at the distances to go a little easier. Short means the DIST was between 1-3 yards. Medium meant 4-7 yards, Long meant 8-10 yards, and very long meant 11+ yards).

With our data collected and gathered, it was time to begin some exploratory research to see if we notice any early patterns in SUCCESS.

## Exploratory Research

This Section is super easy to write up, save this for later so I can focus on the different methods used for now:

## Method 1: Logistic Regression

To start off our model building, I wanted to use a logistic regression model. Logistic regression models are nice because they allow us to look at the relationship between a response variable and however many predictor variables we want to use. Because we are modeling SUCCESS, a categorical variable, we use Logistic Regression instead of Linear. My first step was to build 4 models, one for each variable that I am interested in using: DN, DIST, PERSONNEL, and RP:

```{r}
logRP <- glm(SUCCESSFUL ~ RP, data = all_seasons_logit, family = binomial)
AIC(logRP)

logDIST <- glm(SUCCESSFUL ~ DIST, data = all_seasons_logit, family = binomial)
AIC(logDIST)

logDN <- glm(SUCCESSFUL ~ DN, data = all_seasons_logit, family = binomial)
AIC(logDN)

logPERSONNEL <- glm(SUCCESSFUL ~ PERSONNEL, data = all_seasons_logit, family = binomial)
AIC(logPERSONNEL)
```

With these starter models, the model with the lowest AIC was logDIST (AIC = 1442.7), which uses the DIST variable to predict SUCCESSFUL. We saw a strong relationship with DIST and SUCCESS in our exploratory section, so it makes sense as to why that model had the best AIC. However, we know as we begin to add more variables, we will probably start to see better models. The best way to find the best combination of variables for a logistic regression model would be to create a model with all of our variables, and then use the `step()` function to find which model is the best.

```{r}
logALL <- glm(SUCCESSFUL ~ RP + DIST + DN + PERSONNEL, data = all_seasons_logit, family = binomial)

logBEST <- step(logALL, direction = "backward")

summary(logBEST)
```

LogBEST returned back a model with all four variables with an AIC of 1414.8, which was better than our logDIST model. However, I noticed in the coding output that a model with DN, DIST, and RP (no PERSONNEL) got the same AIC of 1414.8. To decide whether or not to keep the PERSONNEL term, I compared the two models BIC and found that the model without PERSONNEL resulted in a lower BIC. Because of this, my final predictor variables are DN, DIST, and RP.

Next, I used the `train()` function to build the model again, but added 5 fold cross validation. The biggest reason for doing this was to be able to put every model I build through the same validation proccess to make them easy to compare.

```{r}
logFinal <- train(
  SUCCESSFUL ~ RP + DIST + DN,  # your formula
  data = all_seasons_logit,                 # dataset
  method = "glm",                           # logistic regression
  family = binomial,                        # logistic model
  trControl = trainControl(
    method = "cv",                          # k-fold cross-validation
    number = 5,                             # 5 folds
    savePredictions = "final",              # save predictions for held-out folds
    classProbs = TRUE                       # get predicted probabilities
  )
)

```

I then calculated the classification rate for this model. Using DN (Down), DIST (Distance to go), and RP (whether the play was a run or pass) to predict SUCCESS resulted in a classification rate of 0.606, meaning our model correctly predicted 60.6% of the actual plays. I wanted to also visualize this model so I built the following graph.

```{r}

most_common_DN <- names(sort(table(all_seasons_logit$DN), decreasing = TRUE))[1]

dist_values <- sort(unique(all_seasons_logit$DIST))
RP_levels <- levels(all_seasons_logit$RP)

dist_grid <- expand.grid(
  DIST = dist_values,
  RP = RP_levels,
  DN = most_common_DN
)

# Use caret's predict method
dist_grid$pred_prob <- predict(logFinal, newdata = dist_grid, type = "prob")[, "Yes"]

# Plot
ggplot(dist_grid, aes(x = DIST, y = pred_prob, color = RP)) +
  geom_line(size = 1) +
  labs(
    x = "Distance",
    y = "Predicted Probability of Success",
    color = "RP Level",
    title = "Predicted Probability of Success by DIST and RP"
  ) +
  theme_minimal()

```

To be correct 60% of the time is fairly good, however, there are many methods that can be used to predict outcomes. Lets now try Logistic Regression Models that include interaction terms.

## Method 2: Logstic Regression with Interaction

To begin our logistic regression model with interaction, we again build a model with not only all the predictors, but all of the possible interactions as well. We then use the step function to find the best possible model.

```{r}
logALL_inter <- glm(SUCCESSFUL ~ (RP  + DIST + DN + PERSONNEL)^2, data = all_seasons_logit, family = binomial)

logBEST_inter <- step(logALL_inter, direction = "both", trace = FALSE)
summary(logBEST_inter)
```

It seems that the best possible model using our 4 variables to predict success, based on AIC and using interaction terms, is a model with RP, DIST,DN, PERSONNEL, and RP:DIST interaction(AIC = 1406).

Next, I used the `train()` function again to build the model using 5 fold cross validation.

```{r}
LOGINTERACTION <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL + RP*DIST,
  data = all_seasons_logit,                
  method = "glm",                           
  family = binomial,                      
  trControl = trainControl(
    method = "cv",                          # k-fold cross-validation
    number = 5,                             # 5 folds
    savePredictions = "final",              # save predictions for held-out folds
    classProbs = TRUE                       # get predicted probabilities
  )
)

```

After the model goes through 5 fold cross-validaation, I can now calculate the classification rate for this model. A model using DN, DIST, RP, PERSONNEL, and an interaction with RP and DIST to predict SUCCESS resulted in a classification rate of 0.627, meaning our model correctly predicted 62.7% of the actual plays. I wanted to also visualize this model so I built the following graph.

```{r}

most_common_DN <- names(sort(table(all_seasons_logit$DN), decreasing = TRUE))[1]
most_common_PERSONNEL <- names(sort(table(all_seasons_logit$PERSONNEL), decreasing = TRUE))[1]

dist_values <- sort(unique(all_seasons_logit$DIST))
RP_levels <- levels(all_seasons_logit$RP)

dist_grid <- expand.grid(
  DIST = dist_values,
  RP = RP_levels,
  DN = most_common_DN,
  PERSONNEL = most_common_PERSONNEL
)

# Use caret's predict method
dist_grid$pred_prob <- predict(LOGINTERACTION, newdata = dist_grid, type = "prob")[, "Yes"]

# Plot
ggplot(dist_grid, aes(x = DIST, y = pred_prob, color = RP)) +
  geom_line(size = 1) +
  labs(
    x = "Distance",
    y = "Predicted Probability of Success",
    color = "RP Level",
    title = "Logistic Regression Predicted Probability by DIST and RP (Interaction Model)"
  ) +
  theme_minimal()

```

We will compare all four models at the end, however, we do begin to see that a model with interaction is a little better at predicting success than our model without interaction.

## Method 3: Classification Trees

Logistic regression is not the only method that can be used in this scenario. Another effective method that can be used are classification trees. Classification trees is a kind of model that starts with one yes or no question that then breaks off into branches of yes or no questions that best predict what your model is trying to predict. For this paper, the classifcation tree is trying to find what pathways the data best follows to predict whether a play was successful or not. Our model is as follows:

```{r}
ctrl <- trainControl(
  method = "cv",    # cross-validation
  number = 5,       # 5 folds
  classProbs = TRUE, #  predicted probabilities
  summaryFunction = twoClassSummary 
)

```

```{r}
set.seed(123) 

tree <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL,
  data = all_seasons_trees,
  method = "rpart",
  trControl = ctrl,
  metric = "ROC", 
  tuneLength = 10     
)

```

To help further understand what a classification tree looks like, here is the classification tree for my model:

*NEED TO INSERT EITHER THE ENTIRE TREE OR MAYBE A SECTION OF THE TREE*

Since we already built our model with 5 fold cross-validation, we can jump right into finding our classification rate. Classification Rate for our Classification Tree is 0.665, meaning our model correctly predicted 66.5% of the actual plays. This is better than both our base logistic regression model, and our logistic regression model with interaction. Before we do a full comparison of all our models, we will build our final two models using bagging and random forest.

## Method 4: Bagging and Random Forest

Classification trees can be really effective, however,they can sometimes be very sensitive to changes in the training data. That is where bootstrapping comes in. Bootstrapping repeatedly re-samples with replacement your original data, and creates new test data. Then, it runs a new classification tree based on the new test data. It will do this as many times as you would like, helping reduce the variance in your model. Our bootstrapping model is as follows:

```{r}
football_bag2 <- train( 
  SUCCESSFUL ~ `RP` + DIST + DN + PERSONNEL,
  data = all_seasons_bagging, 
  method = "treebag", 
  trControl = trainControl(method = "cv", number = 5), # cv = cross validation, number is amount of folds
  nbagg = 200,  #the number of trees being built
  control = rpart.control(minsplit = 2, cp = 0) #
)

football_bag2
```

We then can use this model and calculate our classifcation rate, which we get to be 0.724. This means our model correctly predicted 72.4% of the actual plays. This is a big jump from our classifcation tree method. Lets see if random forest makes a similar jump.

Random forest

```{r}
ctrl2 <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  allowParallel = FALSE   # <-- this is the fix
)


set.seed(123)

football_rf <- train(
  SUCCESSFUL ~ RP + DIST + DN + PERSONNEL,
  data = all_seasons_bagging,
  method = "ranger",
  trControl = ctrl2,
  metric = "ROC",
  tuneLength = 10
)

```

For our random forest model, we found a classification rate of 0.667. This means our model correctly predicted 72.4% of the actual plays.

## Compare and Contrast the 4 methods\*

To easily compare our 5 classification rates we have found, I created the following table:

```{r}
# Create the data frame
classification_table <- data.frame(
  Method = c(
    "Logistic Regression",
    "Logistic Regression with Interaction",
    "Classification Trees",
    "Bagging",
    "Random Forest"
  ),
  Classification_Rate = c(
    classification_rate,
    classification_rate2,
    classification_rate3,
    classification_rate_bag2,
    classification_rate_rf
  )
)

# Display a clean table
kable(classification_table, 
      col.names = c("Method", "Classification Rate"), 
      digits = 3,        # rounds numbers to 3 decimals
      align = c("l","c"), # left align first column, center second
      caption = "Comparison of Classification Rates") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"),
                full_width = F,
                position = "center")

```

As you can see, the far away best model we built was our bagging model. This makes a lot of sense as bagging uses re sampling and mass testing to help eliminate variance in the data. We notice that logistic regression, with or without interaction, were the worst two models. Logistic regression works best when data has linearity, however, football data can be very nonlinear at times. Big plays can happen that disrupt linearity. Variance in success between games can effect linearity. The different tree-based methods do a much better job at accounting for variance and data spread, specifically bagging and random forest. These models use their own methods to help combat the variance in the data. 


## Conclusion

## References

-   https://www.footballstudyhall.com/2012/2/16/2798555/in-defense-of-success-rates?utm_source=chatgpt.com

## Apendix: this is where i will put all of the coding to my paper so it doesn't interupt with the flow of paper
